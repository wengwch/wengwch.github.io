[ { "title": "protobuf encoding 解析", "url": "/posts/protobuf-encoding/", "categories": "Programing Language", "tags": "Protobuf", "date": "2021-02-12 06:00:00 +0800", "snippet": "Protobuf是谷歌推出的跨语言，跨平台，可扩展的数据序列化机制，相比XML，JSON等序列化方式，它具有更简单，更快，更小的优点，其中体积更小的优点得益于它的encoding方式。Protobuf 消息是一种key-value形式的结构，如下图所示其中Key包含了field_number和wire_type，计算公式：key = (field_number &lt;&lt; 3) | w...", "content": "Protobuf是谷歌推出的跨语言，跨平台，可扩展的数据序列化机制，相比XML，JSON等序列化方式，它具有更简单，更快，更小的优点，其中体积更小的优点得益于它的encoding方式。Protobuf 消息是一种key-value形式的结构，如下图所示其中Key包含了field_number和wire_type，计算公式：key = (field_number &lt;&lt; 3) | wire_typewire_type就是protobuf的编码方式，可用的wire_type和pb对应的数据类型如下: Type Meaning Used For 0 Varint int32, int64, uint32, uint64, sint32, sint64, bool, enum 1 64-bit fixed64, sfixed64, double 2 Length-delimited string, bytes, embedded messages, packed repeated fields 3 Start group groups (deprecated) 4 End group groups (deprecated) 5 32-bit fixed32, sfixed32, float Base 128 VarintsVarint（可变长int类型）使用一个或者多个byte序列化int类型的方法，数字越小，需要的byte越少，类似sql的varchar。Varint 中的每个字节（最后一个字节除外）都设置了最高有效位（msb），用来表示后续还有没有byte数据，剩下的7bits用来存储数据的二进制补码(two’s complement)，因为protobuf采用小端序(little-endian byte order)编码，先读到的是低位数据，所有一个数据如果有多个byte，解析时需要反转一下顺序。看一个例子:message Test1 { optional int32 a = 1;}如果将a设置为150，序列化之后的结果就是 08 96 01 ，解析时先读到一个字节08 ，二进制0000 1000，msb位是0，后续没有数据，因此第一个字节就是key，去除msb后 000 1000， 低位3bit表示wire_type，是0，所以后面的value采用varint编码，右移三位后，剩下的就是field_number，为1。解析完key后，再去解析value，一直读到msb是0的byte停止，解析过程如下96 01 = 1001 0110 0000 0001 → 000 0001 ++ 001 0110 (丢弃msb，保留剩下的 7 bits, 并且翻转顺序) → 10010110 → 128 + 16 + 4 + 2 = 150负数在对负数进行编码的时候，使用signed int类型(sint32 and sint64)和标准类型(int32 and int64)有一些不同，使用int32或者int64时，pb会固定使用10byte来表示负数，而使用signed int类型，会先进行ZigZag编码，再用varint方式序列化。ZigZag编码方式如下: Signed Original Encoded As 0 0 -1 1 1 2 -2 3 2147483647 4294967294 -2147483648 4294967295 ZigZag计算函数如下:(n &lt;&lt; 1) ^ (n &gt;&gt; 31), // n 为 sint32 时(n &lt;&lt; 1) ^ (n &gt;&gt; 63), // n 为 sint64 时需要注意的是，这里的右移操作 (n&gt;&gt;31)是算数移动(arithmetic shift)，如果n是正数，高位补0，如果n是负数，高位补1Non-varint NumbersNon-varint Numbers（fixed64, sfixed64, double, fixed32, sfixed32, float）就比较简单，只要根据wire_type类型读取固定长度的字节数就可以了。Stringswire type是2 (length-delimited)表示Value部分由Length和Data两部分组成，Length是采用varint编码的数字，表示Data的长度，然后就可以读取该长度的数据。实例如下:message Test2 { optional string b = 2;}设置 b = \"testing\"，编码如下:12 07 [74 65 73 74 69 6e 67][ ]中的数据是UTF8 编码的 \"testing\"。 key是0x12，解析如下:0x12→ 0001 0010 (binary representation)→ 00010 010 (regroup bits)→ field_number = 2, wire_type = 2Length是07表示后续7个bytes就是数据部分，直接读取7个字节即可。Embedded MessagesEmbedded Messages的wire type也是2，因此编码方式和string类似，看下例子:message Test3 { optional Test1 c = 3;}设置Test1.a = 150 ，编码如下: 1a 03 08 96 0108 96 01和前面例子是一样的，根据Length 03读取三个字节就得到08 96 01，对这部分数据再进行解析即可。Repeated字段对于repeated字段，解析时会有多个key相同的数据，这些数据不需要连续出现。Packed Repeated Fieldspacked repeated字段会将所有数据打包到一个key-value pair 中，所有数据是连续出现，打包在一起的。message Test4 { repeated int32 d = 4 [packed=true];}设置d = 3, 270, 86942，编码后：22 // key (field number 4, wire type 2)06 // payload size (6 bytes)03 // first element (varint 3)8E 02 // second element (varint 270)9E A7 05 // third element (varint 86942)只有原始数字类型（使用varint，32位或64位）的重复字段才可以声明为“packed”。总结protobuf在序列化方面，相比xml, json等具有更简单，更快，更小的优点，因此可以运用在数据传输（rpc），存储（缓存）等方面，不仅可以提高性能，还能节省大量存储空间，减少成本。参考Protobuf 官方文档。高效的数据压缩编码方式 Protobuftwo’s complementEndianness" }, { "title": "JDK源码学习之LinkedHashMap", "url": "/posts/jdk-linkedhashmap/", "categories": "Programing Language", "tags": "Java", "date": "2017-01-07 22:30:29 +0800", "snippet": "LinkedHashMap继承了HashMap，其行为和HashMap类似，只是在内部多维护了一个元素结点的双向链表。官方文档描述如下： Hash table and linked list implementation of the Map interface, with predictable iteration order. This implementation differs f...", "content": "LinkedHashMap继承了HashMap，其行为和HashMap类似，只是在内部多维护了一个元素结点的双向链表。官方文档描述如下： Hash table and linked list implementation of the Map interface, with predictable iteration order. This implementation differs from HashMap in that it maintains a doubly-linked list running through all of its entries. This linked list defines the iteration ordering, which is normally the order in which keys were inserted into the map (insertion-order). Note that insertion order is not affected if a key is re-inserted into the map. (A key k is reinserted into a map m if m.put(k, v) is invoked when m.containsKey(k) would return true immediately prior to the invocation.)通常情况下，LinkedHashMap可以保持元素的插入顺序。另外，有一个特殊的构造方法，可以使LinkedHashMap保持元素的访问顺序，利用这个特种可以很方便的实现LRUCache。数据结构LinkedHashMap结点结构如下:static class Entry&lt;K,V&gt; extends HashMap.Node&lt;K,V&gt; { Entry&lt;K,V&gt; before, after; Entry(int hash, K key, V value, Node&lt;K,V&gt; next) { super(hash, key, value, next); }}结点继承了HashMap的结点，并且多了before, after两个指针，用来构造双向链表。LinkedHashMap的数据结构大概如下：get, put, remove实现LinkedHashMap的get，put，remove方法的实现和HashMap基本是一样的，只是多了双向链表的维护操作，并不难，不做具体的分析了。只需注意一点即可，就是新插入的结点是放在双向链表的尾部的，如果是保持accessOrder的话，每次结点访问后，也会将结点放到链表的尾部。对于双向链表的维护，主要是通过get，put，remove中的钩子方法afterNodeAccess, afterNodeInsertion, afterNodeRemoval来实现的，在HashMap中这些方法都是空的实现。大概看一下这三个方法的代码:void afterNodeRemoval(Node&lt;K,V&gt; e) { // unlink LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; p.before = p.after = null; if (b == null) head = a; else b.after = a; if (a == null) tail = b; else a.before = b;}void afterNodeInsertion(boolean evict) { // possibly remove eldest LinkedHashMap.Entry&lt;K,V&gt; first; /** * removeEldestEntry方法用来判断是否要删除链表头部的结点，默认为false。 * 我们可以重写该方法，配置accessOrder可以很容易的实现LRUCache */ if (evict &amp;&amp; (first = head) != null &amp;&amp; removeEldestEntry(first)) { K key = first.key; removeNode(hash(key), key, null, false, true); }}void afterNodeAccess(Node&lt;K,V&gt; e) { // move node to last LinkedHashMap.Entry&lt;K,V&gt; last; // accessOrder为true，会将访问的结点放到链表尾部 if (accessOrder &amp;&amp; (last = tail) != e) { LinkedHashMap.Entry&lt;K,V&gt; p = (LinkedHashMap.Entry&lt;K,V&gt;)e, b = p.before, a = p.after; p.after = null; if (b == null) head = a; else b.after = a; if (a != null) a.before = b; else last = b; if (last == null) head = p; else { p.before = last; last.after = p; } tail = p; ++modCount; }}实现LRUCache直接看代码:LinkedHashMap&lt;Integer, Integer&gt; lruCache = new LinkedHashMap&lt;Integer, Integer&gt;(8, 0.75f, true) { @Override protected boolean removeEldestEntry(Map.Entry&lt;Integer, Integer&gt; eldest) { return size() &gt; 5; }};range(0, 100).forEach(value -&gt; lruCache.put(value, value));System.out.println(\"size: \" + lruCache.size());lruCache.keySet().forEach(System.out::println);运行结果:size: 59596979899这里当Map中元素超过5个，就会删除头部的元素，因此map最终只有5个最近插入的元素。总结如果看过HashMap的源码，LinkedHashMap并不难，除了能实现LRU外，LinkedHashMap也能产生一个Map的copy，并且保持原有的顺序。 void foo(Map m) { Map copy = new LinkedHashMap(m); ... }此外，由于LinkedHashMap不是线程安全的，因此并发环境中，可以先将其包装成为同步的。Map m = Collections.synchronizedMap(new LinkedHashMap(...));参考LeetCode – LRU Cache (Java)LinkedHashMap" }, { "title": "Java8之lambda表达式", "url": "/posts/java8-lambda/", "categories": "Programing Language", "tags": "Java", "date": "2016-11-27 06:44:50 +0800", "snippet": "Lambda表达式是Java8最重要也最令人期待的特性，它使得Java初步具有了函数式编程的能力。虽然Lambda表达式的本质只是基于接口的语法糖，但是依然可以给开发带来便利，尤其是Stream的引入，使对集合的操作变的更加方便和强大。lambda 表达式lambda表达式格式: (param1, param2, ...) -&gt; expression, 如果不能用一个表达式表示运行的代...", "content": "Lambda表达式是Java8最重要也最令人期待的特性，它使得Java初步具有了函数式编程的能力。虽然Lambda表达式的本质只是基于接口的语法糖，但是依然可以给开发带来便利，尤其是Stream的引入，使对集合的操作变的更加方便和强大。lambda 表达式lambda表达式格式: (param1, param2, ...) -&gt; expression, 如果不能用一个表达式表示运行的代码，可以使用以下方式:(param1, param2, ...) -&gt; { statement1; statement2; ... statementN; return expr;}函数式接口含有单一方法的接口称为SAM(Single Abstract Method，单抽象方法)接口。函数式接口是对旧有SAM接口的增强，它允许我们用lambda表达式取代传统的匿名类来实例化一个接口。要声明一个函数式接口也很容易，只需在SAM接口加上@FunctionalInterface注解。@FunctionalInterfacepublic interface MyFunction&lt;T&gt; { T apply(T t);}lambda表达式和匿名内部类类似，不过也有一些区别。 匿名内部类访问外部变量时，外部变量需要声明为final。lambda表达式没有这个限制，但是编译器会隐式的把变量当成final的。因此在lambda表达式中不能修改外部变量。 匿名内部类中的this指向该匿名类对象，而lambda中指向外部类对象，类似于闭包的概念。方法引用和构造器引用方法引用使用::操作符将方法名和对象或类名分隔。常用以下三种形式： 对象::实例方法 类::静态方法 类::实例方法前两种情况等同于提供方法参数的lambda表达式。如System.out::println等同于x -&gt; System.out.println(x)，Math::pow等同于(x, y) -&gt; Math.pow(x, y)。第三种情况，第一个参数会成为执行方法的对象。如String::compareTo等同于(x, y) -&gt; x.compareTo(y)。构造器引用和方法引用类似，只不过方法名是new。如Person::new。StreamJava文档对stream包的描述如下: Classes to support functional-style operations on streams of elements, such as map-reduce transformations on collectionsstream支持对元素流进行函数式风格的操作，比如说对集合进行map-reduce的变换。stream和集合类似，但是也有一些不同的特点。 stream 不存储值，只担当从输入源引出的管道角色，一直连接到终结操作上产生输出。 stream 从设计上就偏向函数式风格，避免与状态发生关联。例如 filter() 操作在返回筛选结果的 stream 时，并不会改动底下的集合。 stream的中间操作总是缓求值的 stream 可以没有边界(无限长)。 stream 像 Iterator 的实例一样，也是消耗品，在终结操作之后必须重新生成新的 stream 才能再次操作。stream的操作被分为中间操作(intermediate operations)和终结操作(terminal operations)。中间操作返回一个新的stream并且总是缓求值的，例如filter并不会真的执行过滤操作，只是生成一个新的stream。终结操作遍历stream，产生结果和副作用，如forEach，collect。中间操作又进一步被分为有状态和无状态操作。无状态操作可以独立的处理每个元素，并不依赖之前的元素，如filter，map。而有状态的操作在处理元素时，必须依赖并保存之前元素的状态，如distinct，sorted。此外，有些操作被称为短路操作(short-circuiting operations)。如果一个中间操作是短路的，当它和一个无限输入一起使用时，会产生一个有限结果的stream，如limit。如果一个终结操作是短路的，当它和一个无限输入一起使用时，会在有限时间内完成终结操作。结束这篇仅仅是一些概念的整理和记录，并不涉及lambda表达式以及Stream的具体使用。有兴趣可以自行研究。参考Stream operations and pipelines函数式编程思维写给大忙人看的Java SE 8" }, { "title": "JDK源码学习之ConcurrentHashMap", "url": "/posts/jdk-concurrenthashmap/", "categories": "Programing Language", "tags": "Java", "date": "2016-11-13 19:17:40 +0800", "snippet": "概述上一篇介绍了HashMap的代码实现。众所周知，HashMap不是线程安全的，那么怎样让HashMap变得线程安全呢，最简单的方法就是给HashMap的方法加上synchronized，使其变为同步的，HashTable就是这么干的。这样虽然能解决并发的问题，但是由于锁粒度太大，多线程时竞争激烈，效率很低，因此并不推荐使用。在并发环境下，有更好的选择ConcurrentHashMapCo...", "content": "概述上一篇介绍了HashMap的代码实现。众所周知，HashMap不是线程安全的，那么怎样让HashMap变得线程安全呢，最简单的方法就是给HashMap的方法加上synchronized，使其变为同步的，HashTable就是这么干的。这样虽然能解决并发的问题，但是由于锁粒度太大，多线程时竞争激烈，效率很低，因此并不推荐使用。在并发环境下，有更好的选择ConcurrentHashMapConcurrentHashMap在java7中使用分段锁的机制减小锁粒度，提高并发效率。但是在java8中，已经抛弃了这种实现，使用更加高效的CAS自旋的方式进一步提升HashMap在并发环境中的效率。这篇主要介绍一下put和resize过程的实现。关键字段和方法在看源代码之前，先了解一下ConcurrentHashMap的几个关键字段和方法。先看一下几个关键的字段。/** * Node数组，长度为2的指数次方，用于存储数据。在第一次插入时初始化 */private transient volatile Node&lt;K,V&gt;[] table;/** * resize过程中用到，只有在resize过程中非空 */private transient volatile Node&lt;K,V&gt;[] nextTable;/** * 计数器，主要在没有竞争的时候使用 * 有竞争时，使用类似LongAdder的机制提高并发环境下计数器的效率 */private transient volatile long baseCount;/** * 用于控制table的初始化和resize过程 * -1 表示正在初始化过程中 * 其他负数表示正在resize过程中 * 除此之外，值为0.75*table.length */private transient volatile int sizeCtl;/** * 在resize过程中，分割table，使多线程可以同时对table的不同区间进行resize操作，提高resize的效率 */private transient volatile int transferIndex;注意到，这些字段都有volatile关键字，主要用于保证内存可见性和顺序性。关于volatile关键字的作用，有兴趣的可以自己去研究一下。再看一下三个关键方法// 获取table[i]的值static final &lt;K,V&gt; Node&lt;K,V&gt; tabAt(Node&lt;K,V&gt;[] tab, int i);// 使用cas设置table[i]的值，成功返回true，失败返回falsestatic final &lt;K,V&gt; boolean casTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; c, Node&lt;K,V&gt; v)// 设置 table[i]的值static final &lt;K,V&gt; void setTabAt(Node&lt;K,V&gt;[] tab, int i, Node&lt;K,V&gt; v)这三个操作都是原子的，可以确保对table的修改都是原子操作。put实现ConcurrentHashMap的put实现和HashMap的很相似，主要区别就是使用CAS使对表的操作原子化，以确保线程安全。直接看代码 final V putVal(K key, V value, boolean onlyIfAbsent) { if (key == null || value == null) throw new NullPointerException(); // 计算key的hash值 int hash = spread(key.hashCode()); int binCount = 0; for (Node&lt;K, V&gt;[] tab = table; ; ) { Node&lt;K, V&gt; f; int n, i, fh; if (tab == null || (n = tab.length) == 0) // 初始化table tab = initTable(); else if ((f = tabAt(tab, i = (n - 1) &amp; hash)) == null) { // 散列位置为空，存入Node，如果返回false，说明有多个线程尝试修改改位置，自旋后重新尝试修改 if (casTabAt(tab, i, null, new Node&lt;K, V&gt;(hash, key, value, null))) break; // no lock when adding to empty bin } else if ((fh = f.hash) == MOVED) // 如果哈希值为MOVED，表明这是一个ForwardingNode，其他线程正在进行resize操作，去帮助resize tab = helpTransfer(tab, f); else { // 以下过程和HashMap类似，如果key值存在，就覆盖旧值，不存在，就插入到链表尾部 V oldVal = null; synchronized (f) { if (tabAt(tab, i) == f) { if (fh &gt;= 0) { binCount = 1; for (Node&lt;K, V&gt; e = f; ; ++binCount) { K ek; if (e.hash == hash &amp;&amp; ((ek = e.key) == key || (ek != null &amp;&amp; key.equals(ek)))) { oldVal = e.val; if (!onlyIfAbsent) e.val = value; break; } Node&lt;K, V&gt; pred = e; if ((e = e.next) == null) { pred.next = new Node&lt;K, V&gt;(hash, key, value, null); break; } } } else if (f instanceof TreeBin) { Node&lt;K, V&gt; p; binCount = 2; if ((p = ((TreeBin&lt;K, V&gt;) f).putTreeVal(hash, key, value)) != null) { oldVal = p.val; if (!onlyIfAbsent) p.val = value; } } } } if (binCount != 0) { if (binCount &gt;= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; } } } // 计数计加1，并会检查是否需要resize addCount(1L, binCount); return null; }resize实现resize的大概流程是这样的。从旧表的尾部往前寻找未处于resize过程的区间，将区间开始位置的结点都转移到新表中，并将该位置设置为ForwardingNode，表明该区间处于resize过程。然后从后往前遍历该区间，将所有位置都设置为ForwardingNode，并所有结点都转移到新表中。完成该区间的resize操作后，再对下一区间进行resize操作，直到旧表中所有结点都转移到新表中，用新表取代旧表。由于resize是以区间为单位进行的，因此多线程可以同时对不同的区间进行resize操作，不会有数据冲突，可以提高整个表resize的速度。下面是简单的图例。寻找未处于resize过程的区间转移区间开始位置的结点到新表，并设置该位置为ForwardingNode，表明区间正处于resize过程遍历该区间，将所有结点都转移到新表中下面看一下代码的实现细节private final void transfer(Node&lt;K, V&gt;[] tab, Node&lt;K, V&gt;[] nextTab) { int n = tab.length, stride; // 设置区间长度，最小为16 if ((stride = (NCPU &gt; 1) ? (n &gt;&gt;&gt; 3) / NCPU : n) &lt; MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range // nextTab为空，则创建新表。已经有其他线程正在进行resize操作，则不为空 if (nextTab == null) { // initiating try { @SuppressWarnings(\"unchecked\") Node&lt;K, V&gt;[] nt = (Node&lt;K, V&gt;[]) new Node&lt;?, ?&gt;[n &lt;&lt; 1]; nextTab = nt; } catch (Throwable ex) { // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; } nextTable = nextTab; transferIndex = n; } int nextn = nextTab.length; ForwardingNode&lt;K, V&gt; fwd = new ForwardingNode&lt;K, V&gt;(nextTab); boolean advance = true; boolean finishing = false; // to ensure sweep before committing nextTab for (int i = 0, bound = 0; ; ) { Node&lt;K, V&gt; f; int fh; // 寻找没有处于resize过程的区间 while (advance) { int nextIndex, nextBound; // --i，用于遍历区间，或者在resize完成时，遍历整个表检查是否将所有结点都转移 if (--i &gt;= bound || finishing) advance = false; else if ((nextIndex = transferIndex) &lt;= 0) { i = -1; advance = false; } else if (U.compareAndSwapInt(this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex &gt; stride ? nextIndex - stride : 0))) { // 设置区间的开始结束位置bound 和 nextIndex bound = nextBound; i = nextIndex - 1; advance = false; } } if (i &lt; 0 || i &gt;= n || i + n &gt;= nextn) { int sc; if (finishing) { // resize 完成，使用新表取代旧表 nextTable = null; table = nextTab; // 设置sizeCtl为0.75倍的新表长度 sizeCtl = (n &lt;&lt; 1) - (n &gt;&gt;&gt; 1); return; } if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) { // 完成resize，线程退出。最后一个完成resize的线程会进行rechek，查看所有结点是否全部转移，并将没有转移的都转移到新表，然后使用新表取代旧表 if ((sc - 2) != resizeStamp(n) &lt;&lt; RESIZE_STAMP_SHIFT) return; finishing = advance = true; i = n; // recheck before commit } } else if ((f = tabAt(tab, i)) == null) // 该位置没有结点，直接设置为ForwardingNode advance = casTabAt(tab, i, null, fwd); else if ((fh = f.hash) == MOVED) // 该结点所处区间已经处于resize过程，寻找下一个区间 advance = true; // already processed else { synchronized (f) { // rehash过程，将结点转移到新表上，和HashMap基本一致 if (tabAt(tab, i) == f) { Node&lt;K, V&gt; ln, hn; if (fh &gt;= 0) { int runBit = fh &amp; n; Node&lt;K, V&gt; lastRun = f; for (Node&lt;K, V&gt; p = f.next; p != null; p = p.next) { int b = p.hash &amp; n; if (b != runBit) { runBit = b; lastRun = p; } } if (runBit == 0) { ln = lastRun; hn = null; } else { hn = lastRun; ln = null; } for (Node&lt;K, V&gt; p = f; p != lastRun; p = p.next) { int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph &amp; n) == 0) ln = new Node&lt;K, V&gt;(ph, pk, pv, ln); else hn = new Node&lt;K, V&gt;(ph, pk, pv, hn); } // 部分结点转移的新表不变的位置 setTabAt(nextTab, i, ln); // 部分结点转移到新表原位置+旧表长度的位置 setTabAt(nextTab, i + n, hn); // 将旧表该位置设置为ForwardingNode，表明已经完成结点转移 setTabAt(tab, i, fwd); advance = true; } else if (f instanceof TreeBin) { //树化的相关操作，省略 ... } } } } }}总结这篇主要介绍了一下ConcurrentHashMap的实现方式，并对ConcurrentHashMap在并发环境中为什么高效做了一些简单的分析。另外从ConcurrentHashMap的实现也能简单了解一些lock free算法的设计和实现思路。参考无锁HashMap的原理与实现无锁队列的实现聊聊并发（四）——深入分析ConcurrentHashMap" }, { "title": "JDK源码学习之HashMap", "url": "/posts/jdk-hashmap/", "categories": "Programing Language", "tags": "Java", "date": "2016-11-06 04:45:01 +0800", "snippet": "1. 概述HashMap是Java开发中最常用的数据结构，阅读源码有助于我们了解其工作原理及实现。关于HashMap的特性，可以参考官方文档： Hash table based implementation of the Map interface. This implementation provides all of the optional map operations, and p...", "content": "1. 概述HashMap是Java开发中最常用的数据结构，阅读源码有助于我们了解其工作原理及实现。关于HashMap的特性，可以参考官方文档： Hash table based implementation of the Map interface. This implementation provides all of the optional map operations, and permits null values and the null key. (The HashMap class is roughly equivalent to Hashtable, except that it is unsynchronized and permits nulls.) This class makes no guarantees as to the order of the map; in particular, it does not guarantee that the order will remain constant over time.HashMap实现了Map接口，允许键/值为null，非线程安全的，也不能保证顺序。HashMap基于数组实现，并使用拉链法解决哈希碰撞的问题。在开始之前，先介绍一下HashMap中的capacity和loadFactor。 capacity表示容量，即hashmap中table数组的长度，该值一直为$2^n$(1&lt;&lt;n)，默认为16。 loadFactor表示负载因子，默认为0.75，当hashmap中元素超过capacity*loadFactor这个阈值，就会进行扩容resize。 HashMap的散列函数采用除留余数法，使用key的hash值被table数组的长度除后所得的余数为散列地址。即\\(f(k) = key.hash\\mod capacity\\)下面看一下在Java8中HashMap的代码实现（主要介绍put, get, resize的实现）。2. putput函数的逻辑大概如下： 令n = table.length, hash = hash(key) 计算结点散列地址index = hash % n 检测是否与table[index]发生碰撞，有碰撞跳转5 直接将结点放入table[index]，跳转7 检查table[index]处的链表是否已经树化，是就将结点插入树中，如果key值存在，直接覆盖旧值，跳转8，如果不存在，跳转7 遍历链表，如果key值存在，直接覆盖旧值，跳转8。如果不存在，将结点插入链表尾部，并检查链表长度是否超过树化的阈值（默认8），超过就将链表转化为红黑树，提高查找效率 检查元素数量是否超过capacity*loadFactor，是就进行扩容并且rehash 结束现在来看一下put的代码：final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { Node&lt;K, V&gt;[] tab; Node&lt;K, V&gt; p; int n, i; if ((tab = table) == null || (n = tab.length) == 0) // 初始化table n = (tab = resize()).length; if ((p = tab[i = (n - 1) &amp; hash]) == null) // 无碰撞 tab[i] = newNode(hash, key, value, null); else { // 碰撞 Node&lt;K, V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; else if (p instanceof TreeNode) // 已经树化，使用putTreeVal插入结点 e = ((TreeNode&lt;K, V&gt;) p).putTreeVal(this, tab, hash, key, value); else { // 遍历链表 for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { // 将结点插入链表尾部 p.next = newNode(hash, key, value, null); if (binCount &gt;= TREEIFY_THRESHOLD - 1) // -1 for 1st // 将链表转化为红黑树，提高效率 treeifyBin(tab, hash); break; } if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; } } if (e != null) { // existing mapping for key // key值已经存在 V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) // 覆盖旧值 e.value = value; afterNodeAccess(e); return oldValue; } } // 修改计数，在使用迭代器的过程中有其他线程修改了map，那么将抛出ConcurrentModificationException ++modCount; if (++size &gt; threshold) // 扩容 resize(); afterNodeInsertion(evict); return null; } 3. resizeresize做了两件事情： 将table容量扩大到两倍大小 rehash计算元素在新table中的散列地址，存入新table扩容很简单，直接创建一个原table大小2倍的新table即可。对于rehash计算的新散列地址，只有两种情况： 新地址不变 新地址 = 旧地址 + 扩容前表容量大小以上结论很容易推导：假设原表容量为n，\\(n=2^k\\)， 扩容后新表容量为\\(m = 2n =2^{k + 1}\\)，key的hash值为hrehash前： 令\\(a = \\lfloor{\\frac h n}\\rfloor = \\lfloor{\\frac h {2^k}}\\rfloor\\) 散列地址为\\(f_1(h) = h\\mod n = h - an\\)rehash后： 散列地址为\\(f_2(h) = h\\mod m = h\\mod 2n = h -\\lfloor \\frac a 2 \\rfloor m\\)如果a为偶数，\\(f_2(h) = h - \\lfloor \\frac a 2 \\rfloor m = h - \\frac a 2 2n = h - an = f_1(h)\\)如果a为奇数，\\(f_2(h) = h - \\lfloor \\frac a 2 \\rfloor m = h - \\frac {a - 1} 2 2n = h - an + n = f_1(h) + n\\)了解以上结论，再看resize的代码就很容易了。// 创建新表，容量为旧表的两倍Node&lt;K, V&gt;[] newTab = (Node&lt;K, V&gt;[]) new Node[newCap];table = newTab;if (oldTab != null) { //rehash for (int j = 0; j &lt; oldCap; ++j) { Node&lt;K, V&gt; e; if ((e = oldTab[j]) != null) { oldTab[j] = null; if (e.next == null) // 该散列地址只有一个结点，没有碰撞，直接将结点rehash后存入新表 newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) // 对于已经树化的，通过split将结点重新rehash，有兴趣的可以自己研究一下 ((TreeNode&lt;K, V&gt;) e).split(this, newTab, j, oldCap); else { // preserve order Node&lt;K, V&gt; loHead = null, loTail = null; Node&lt;K, V&gt; hiHead = null, hiTail = null; Node&lt;K, V&gt; next; do { next = e.next; if ((e.hash &amp; oldCap) == 0) { // rehash后散列地址不变的结点组成新链表loHead if (loTail == null) loHead = e; else loTail.next = e; loTail = e; } else { // rehash后散列地址改变的结点组成新链表hiHead if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; } } while ((e = next) != null); if (loTail != null) { // 将loHead链表放入新表原地址中 loTail.next = null; newTab[j] = loHead; } if (hiTail != null) { // 将hiHead链表放入新表原地址 + oldCap位置中 hiTail.next = null; newTab[j + oldCap] = hiHead; } } } }}4. get了解了put和resize，get就很容易理解了，直接看代码:final Node&lt;K, V&gt; getNode(int hash, Object key) { Node&lt;K, V&gt;[] tab; Node&lt;K, V&gt; first, e; int n; K k; if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) { // 检查第一个结点，如果key相等，直接返回value if (first.hash == hash &amp;&amp; // always check first node ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))) return first; if ((e = first.next) != null) { if (first instanceof TreeNode) // 已经树化的，在树中查找 return ((TreeNode&lt;K, V&gt;) first).getTreeNode(hash, key); do { // 遍历链表查找，查找key值命中的value if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; } while ((e = e.next) != null); } } return null; }5. 结论HashMap是java开发中最常用的数据结构，也是java面试中常见的问题，了解其源码不管从哪方面看都是有好处的。另外，在平时的使用中，需要注意HashMap并不是线程安全的，在并发环境中使用会出现很多问题，比如resize可以造成死循环等。在并发环境中一定要使用ConcurrentHashMap。参考疫苗：Java HashMap的死循环Java HashMap工作原理及实现Java HashMap工作原理" } ]
